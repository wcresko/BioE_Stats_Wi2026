---
title: "Statistics for Bioengineering - Week 2"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2026 - Knight Campus 
    transition: fade
    transition-speed: slow
    theme: default
    chalkboard: true
    slide-number: true
    code-fold: false
    code-overflow: wrap
    highlight-style: github
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

```{r}
#| label: setup
#| include: false

# Core data manipulation and visualization
library(tidyverse)
library(knitr)
library(readxl)

# Statistical analysis packages
library(MASS)        # LDA, robust regression
library(pwr)         # Power analysis
library(boot)        # Bootstrap methods
library(car)         # Companion to Applied Regression
library(survival)    # Survival analysis
library(nlme)        # Mixed effects models
library(broom)       # Tidy model output
library(emmeans)     # Estimated marginal means

# Set consistent theme for all plots
theme_set(theme_minimal(base_size = 14))

# Set seed for reproducibility
set.seed(2026)
```

# Week 2: EDA, Probability, and Experimental Design {background-color="#2c3e50"}

## Week 2 Topics

::: incremental
-   Exploratory data analysis
-   Data visualization
-   Parameters & statistics
-   Probability distributions
-   Estimates & confidence intervals
:::

::: callout-note
**Readings:** Chapters 9-15
:::

## Packages for This Week {.smaller}

```{r}
#| label: week2-packages-install
#| eval: false
#| echo: true

# Install new packages (run once)
install.packages("nycflights13")

# Load packages
library(tidyverse)    # ggplot2, dplyr, tidyr for data wrangling & visualization
library(nycflights13) # NYC flights data for dplyr examples
```

::: callout-note
**Dataset from package:** `flights` (336,776 flights departing NYC in 2013) - used for dplyr practice
:::

# Exploratory Data Analysis {background-color="#2c3e50"}

## What is EDA? {.smaller}

-   First step in any data analysis
-   Understand structure and patterns in your data
-   Identify outliers, missing values, errors
-   Generate hypotheses for formal testing

::: callout-tip
## Always visualize your data before running statistical tests!
:::

# Data Wrangling with Tidyverse {background-color="#2c3e50"}

## Tidyverse Family of Packages {.smaller}

![Tidyverse packages overview](images/week02_10_tidyverse_packages.jpeg){#fig-week2-tidyverse fig-align="center" width="90%"}

```{r}
#| label: load-tidyverse
#| echo: true
#| output: false

library(tidyverse)
```

## A tibble - a tidyverse dataframe {.smaller}

![](images/week02_11_mpg_tibble_variable_definitions.jpeg){fig-align="center" width="90%"}

## A tibble - a tidyverse dataframe {.smaller}

![](images/week02_12_tidy_data_variables_observations.jpeg){fig-align="center" width="90%"}

## Types of variables in a tibble {.smaller}

![dplyr functions](images/week02_13_r_vector_types_hierarchy.jpeg){#fig-week2-dplyr-functions fig-align="center" width="90%"}

## Key dplyr Verbs {.smaller}

-   `dpylr` is a package in the `tidyverse` that gives you functions (aka verbs) to wrangle data.

| Verb          | Purpose                | Example                         |
|:--------------|:-----------------------|:--------------------------------|
| `filter()`    | Subset rows by values  | `filter(df, x > 5)`             |
| `select()`    | Subset columns by name | `select(df, col1, col2)`        |
| `arrange()`   | Reorder rows           | `arrange(df, x)`                |
| `mutate()`    | Create new columns     | `mutate(df, z = x + y)`         |
| `summarise()` | Collapse to summary    | `summarise(df, mean = mean(x))` |

## Filter, Arrange, and Select {.smaller}

```{r}
#| label: week2-filter-arrange-select
#| eval: false
#| echo: true

# Filter rows
filter(flights, month == 11 | month == 12)

# Arrange rows
arrange(flights, year, month, day)

# Select columns
select(flights, year, month, day)
```

![](images/week02_14_boolean_set_operations_venn.jpeg){fig-align="center" width="50%"}

## Conditional Operators {.smaller}

| Operator  | Meaning                 |
|:----------|:------------------------|
| `==`      | Equals exactly          |
| `<`, `<=` | Less than (or equal)    |
| `>`, `>=` | Greater than (or equal) |
| `!=`      | Not equal to            |
| `!`       | NOT operator            |
| `&`       | AND operator            |
| `|`       | OR operator             |
| `%in%`    | Belongs to set          |

## Mutate and Transmute {.smaller}

```{r}
#| label: week2-mutate-example
#| eval: false
#| echo: true

# Add new columns based on existing ones
mutate(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)

# Using pipes
flights |>
  mutate(
    gain = arr_delay - dep_delay,
    speed = distance / air_time * 60
  )
```

## Group By and Summarise {.smaller}

```{r}
#| label: week2-groupby-summarise
#| eval: false
#| echo: true

# Group by categorical variable
by_day <- group_by(flights, year, month, day)

# Summarise within groups
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

::: callout-warning
Aggregation functions return NA if any input value is NA. Use `na.rm = TRUE` to remove missing values.
:::

## Other Useful dplyr Functions {.smaller}

| Function     | Purpose                  |
|:-------------|:-------------------------|
| `slice()`    | Subset rows by position  |
| `pull()`     | Extract column as vector |
| `count()`    | Count observations       |
| `distinct()` | Unique observations      |

## R Exercise: Data Wrangling {.smaller}

::: callout-tip
## Exercise

1.  Read in `Week1b_Stickle_RNAseq.tsv` or `knee_injury.csv` dataset
2.  `Select` a subset of categorical variables + quantitative variables
3.  `Mutate` to create square root transformed versions
4.  `Summarise` mean and SD `grouped by` categories (such as sex, population, treatment)
5.  Write results to a `.csv` file
:::

# Data Visualization with `ggplot2` {background-color="#2c3e50"}

## Introduction to `ggplot2` {.smaller}

-   Part of the `tidyverse` suite of packages
-   GG stands for "Grammar of Graphics"
-   Start with `ggplot()`, supply dataset and aesthetic mapping with `aes()`
-   Add geometry layers with `geom_*()` functions

::: callout-tip
More info: <https://ggplot2.tidyverse.org/>
:::

## Grammar of Graphics

![](images/week02_27_grammar_of_graphics_layers.jpeg){fig-align="center" width="90%"}

## Scatterplots {.smaller}

```{r}
#| label: ggplot-scatter-output
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4

ggplot(mpg, aes(displ, hwy, color = class)) +
  geom_point(size = 4, alpha = 0.6)
```

## Scatterplots Code {.smaller}

```{r}
#| label: ggplot-scatter-code
#| echo: true
#| eval: false

ggplot(mpg, aes(displ, hwy, color = class)) +
  geom_point(size = 4, alpha = 0.6)
```

## Boxplots {.smaller}

![](images/week02_28_boxplot.jpeg){fig-align="center" width="90%"}

## Boxplots {.smaller}

```{r}
#| label: ggplot-boxplot-output
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4

ggplot(mpg, aes(manufacturer, hwy, colour = class)) +
  geom_boxplot() +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

## Boxplots Code {.smaller}

```{r}
#| label: ggplot-boxplot-code
#| echo: true
#| eval: false

ggplot(mpg, aes(manufacturer, hwy, colour = class)) +
  geom_boxplot() +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

## Common ggplot Geoms {.smaller}

| Geom               | Purpose      | Data Type               |
|:-------------------|:-------------|:------------------------|
| `geom_point()`     | Scatterplots | Two continuous          |
| `geom_line()`      | Line plots   | Continuous over ordered |
| `geom_bar()`       | Bar charts   | Categorical counts      |
| `geom_histogram()` | Histograms   | Single continuous       |
| `geom_boxplot()`   | Boxplots     | Continuous by category  |
| `geom_smooth()`    | Trend lines  | Two continuous          |

## Combining Geoms {.smaller}

```{r}
#| label: combine-geoms-output
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4

ggplot(data=mpg, aes(x=displ, y=hwy)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Fuel efficiency decreases with engine size",
       caption = "Data from fueleconomy.gov")
```

## Combining Geoms Code {.smaller}

```{r}
#| label: combine-geoms-code
#| echo: true
#| eval: false

ggplot(data=mpg, aes(x=displ, y=hwy)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Fuel efficiency decreases with engine size",
       caption = "Data from fueleconomy.gov")
```

## Faceting {.smaller}

```{r}
#| label: facet-example-output
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4

ggplot(data=mpg) +
  geom_point(mapping=aes(x=displ, y=hwy)) +
  facet_wrap(~class, nrow=2)
```

## Faceting Code {.smaller}

```{r}
#| label: facet-example-code
#| echo: true
#| eval: false

ggplot(data=mpg) +
  geom_point(mapping=aes(x=displ, y=hwy)) +
  facet_wrap(~class, nrow=2)
```

## Three-Dimensional Data {.smaller}

```{r}
#| label: 3d-contour-output
#| echo: false
#| eval: true
#| fig-width: 6
#| fig-height: 5

set.seed(345)
d <- data.frame(a = rnorm(100, 10, 10), b = rnorm(100, 5, 5))
ggplot(d, aes(x=a, y=b)) +
  geom_density2d_filled() +
  theme_minimal()
```

## Three-Dimensional Data Code {.smaller}

```{r}
#| label: 3d-contour-code
#| echo: true
#| eval: false

set.seed(345)
d <- data.frame(a = rnorm(100, 10, 10), b = rnorm(100, 5, 5))
ggplot(d, aes(x=a, y=b)) +
  geom_density2d_filled() +
  theme_minimal()
```

## Choosing the Right Plot

![](images/week02_33_flow_chart_for_choosing_data_v.jpeg){fig-align="center" width="90%"}

## R Exercise: Data Visualization {.smaller}

::: callout-tip
## Exercise

1.  Read in `Week1b_Stickle_RNAseq.tsv` or `knee_injury.csv` dataset
2.  Create `histograms` of data
3.  Create histograms on a `faceted` figure for different levels of a category
4.  Repeat steps 2 and 3 but create `boxplots`
5.  Write plots to a `.pdf` file
:::

# Best Practices in Data Visualization {background-color="#2c3e50"}

## Good, Bad, and Ugly {.smaller}

![A ticker tape parade of confusion](images/week02_34_bad_ticker_tape_parade_of_data.jpeg){fig-align="center" width="75%"}

## A line to no understanding

![](images/week02_35_bad_a_line_to_no_understanding.jpeg){fig-align="center" width="75%"}

## Wack a mole {.smaller}

![](images/week02_48_displaying_temporal_patterns.jpeg){fig-align="center" width="80%"}

## Disk of disinformation {.smaller}

![](images/week02_46_choosing_appropriate_bin_width.jpeg){fig-align="center" width="80%"}

## Steaming pie chart mess {.smaller}

![](images/week02_47_relationships_between_variable.jpeg){fig-align="center" width="80%"}

## A bake sale of pie charts

![](images/week02_36_bad_a_bake_sale_of_pie_charts.jpeg){fig-align="center" width="75%"}

##  {.smaller}

![Minard's map of Napoleon's Russian campaign - Edward Tufte](images/week02_37_minards_map_of_napoleons_russi.jpeg){fig-align="center" width="80%"}

##  {.smaller}

![Modern recreation of Minard-style data visualization](images/week02_38_modern_recreation_of_minard_st.png){fig-align="center" width="80%"}

## Principles of Effective Display {.smaller}

> "Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space"
>
> --- Edward Tufte

-   Show the data
-   Encourage the eye to compare differences
-   Represent magnitudes honestly and accurately
-   Draw graphical elements clearly, minimizing clutter
-   Make displays easy to interpret

## "Above All Else Show the Data" {.smaller}

![](images/week02_39_tufte_1983.jpeg){#fig-align="center" width="80%"}

## "Maximize the Data to Ink Ratio" {.smaller}

![The Economist 2006](images/week02_40_the_economist_2006.jpeg){fig-align="center" width="80%"}

## Represent Magnitudes Honestly {.smaller}

![Rattenborg et al. 1999 Nature](images/week02_41_rattenborg_et_al_1999_nature.jpeg){fig-align="center" width="70%"}

## How NOT to Make a Figure {.smaller}

::::: columns
::: {.column width="50%"}
![](images/week02_42_misleading_chart_truncated_yaxis.jpeg){fig-align="center"}
:::

::: {.column width="50%"}
![](images/week02_43_correct_unemployment_line_plot.jpeg){fig-align="center"}
:::
:::::

"Graphical excellence begins with telling the truth about the data" – Tufte 1983

# Parameters and Statistics {background-color="#2c3e50"}

## Population vs. Sample {.smaller}

| Concept    | Population           | Sample               |
|:-----------|:---------------------|:---------------------|
| Definition | All individuals      | Subset of population |
| Parameters | μ (mean), σ (SD)     | x̄ (mean), s (SD)     |
| Goal       | What we want to know | What we can measure  |

::: callout-important
We use sample statistics to **estimate** population parameters
:::

## Accuracy vs. Precision {.smaller}

![](images/week02_52_precision_vs_accuracy_target.jpeg){fig-align="center" width="50%"}

-   **Accuracy**: closeness to true value
-   **Precision**: closeness of repeated estimates to each other
-   **Replication** quantifies variation
-   **Randomization** avoids bias

## Stochastic Processes in Statistics {.smaller}

-   We often want to know truths about the world, but can only estimate them
-   Uncertainty in those estimates is a given
-   Statistics is largely about quantifying and managing uncertainty
-   Random variables are products of stochastic processes
-   Expectations are based on theoretical probability distributions

## Two Interpretations of Probability {.smaller}

**Frequency interpretation:**

"Probabilities are mathematically convenient approximations to long run relative frequencies."

**Subjective (Bayesian) interpretation:**

"A probability statement expresses the opinion of some individual regarding how certain an event is to occur."

## Random Variables and Probability {.smaller}

-   **Probability** is the expression of belief in some future outcome
-   A **random variable** can take on different values with different probabilities
-   The **sample space** is the universe of all possible values
-   Sample space represented by:
    -   **Probability mass distribution** (discrete)
    -   **Probability density function** (continuous)

::: callout-important
Probabilities of a sample space **always sum to 1.0**
:::

## Probability Rules {.smaller}

**'And' rule (multiplication):** $$Pr(X \text{ and } Y) = Pr(X) \times Pr(Y)$$

**'Or' rule (addition):** $$Pr(X \text{ or } Y) = Pr(X) + Pr(Y)$$

::: callout-note
The 'and' rule assumes independent events. For non-independent events, we need conditional probabilities.
:::

## Joint and Conditional Probability {.smaller}

**Joint probability (independent events):** $$Pr(X,Y) = Pr(X) \times Pr(Y)$$

**Conditional probability (independent):** $$Pr(Y|X) = Pr(Y) \text{ and } Pr(X|Y) = Pr(X)$$

**Conditional probability (non-independent):** $$Pr(Y|X) \neq Pr(Y) \text{ and } Pr(X|Y) \neq Pr(X)$$

## Likelihood vs. Probability {.smaller}

-   **Probability**: proportion of times an event would occur over many trials
-   **Likelihood**: conditional probability of a parameter value given data

$$L[\text{parameter}|\text{data}] = Pr[\text{data}|\text{parameter}]$$

-   **Maximum likelihood**: highest value of the likelihood function
-   **Bayesian estimate**: uses prior distribution to update posterior distribution

## Moments of Distributions {.smaller}

**1st moment (mean/expectation):** $$E[X] = \sum_{\text{all x}}xP(X=x) = \mu$$

**2nd moment (variance):** $$Var(X) = E[(X-\mu)^2] = \sigma^2$$

**Standard deviation:** $$SD = \sqrt{\sigma^2} = \sigma$$

Higher moments include skewness (3rd) and kurtosis (4th).

# Discrete Probability Distributions {background-color="#2c3e50"}

## PMF, PDF, and CDF in R {.smaller}

```{r}
#| label: fig-pmf-pdf-cdf-output
#| fig-cap: "Comparing discrete (Poisson) and continuous (Normal) distributions (probability mass function, probability density function, and cumulative density function)"
#| fig-width: 10
#| fig-height: 5
#| echo: false
#| eval: true

par(mfrow = c(2, 2))

# Discrete (Poisson) PMF
x_disc <- 0:15
plot(x_disc, dpois(x_disc, lambda = 5), type = "h", lwd = 3, col = "steelblue",
     main = "PMF: Poisson(λ=5)", xlab = "x", ylab = "P(X = x)")
points(x_disc, dpois(x_disc, lambda = 5), pch = 19, col = "steelblue")

# Continuous (Normal) PDF
x_cont <- seq(-4, 4, length.out = 200)
plot(x_cont, dnorm(x_cont), type = "l", lwd = 2, col = "coral",
     main = "PDF: Normal(0, 1)", xlab = "x", ylab = "f(x)")

# Poisson CDF
plot(x_disc, ppois(x_disc, lambda = 5), type = "s", lwd = 2, col = "steelblue",
     main = "CDF: Poisson(λ=5)", xlab = "x", ylab = "F(x)")

# Normal CDF
plot(x_cont, pnorm(x_cont), type = "l", lwd = 2, col = "forestgreen",
     main = "CDF: Normal(0, 1)", xlab = "x", ylab = "F(x) = P(X ≤ x)")
abline(h = c(0.025, 0.975), lty = 2, col = "gray")
```

## PMF, PDF, and CDF Code {.smaller}

```{r}
#| label: fig-pmf-pdf-cdf-code
#| echo: true
#| eval: false

par(mfrow = c(2, 2))

# Discrete (Poisson) PMF
x_disc <- 0:15
plot(x_disc, dpois(x_disc, lambda = 5), type = "h", lwd = 3, col = "steelblue",
     main = "PMF: Poisson(λ=5)", xlab = "x", ylab = "P(X = x)")
points(x_disc, dpois(x_disc, lambda = 5), pch = 19, col = "steelblue")

# Continuous (Normal) PDF
x_cont <- seq(-4, 4, length.out = 200)
plot(x_cont, dnorm(x_cont), type = "l", lwd = 2, col = "coral",
     main = "PDF: Normal(0, 1)", xlab = "x", ylab = "f(x)")

# Poisson CDF
plot(x_disc, ppois(x_disc, lambda = 5), type = "s", lwd = 2, col = "steelblue",
     main = "CDF: Poisson(λ=5)", xlab = "x", ylab = "F(x)")

# Normal CDF
plot(x_cont, pnorm(x_cont), type = "l", lwd = 2, col = "forestgreen",
     main = "CDF: Normal(0, 1)", xlab = "x", ylab = "F(x) = P(X ≤ x)")
abline(h = c(0.025, 0.975), lty = 2, col = "gray")
```

## Common moments of distributions

![](images/week02_80_population_sample_statistics_table.jpeg){fig-align="center" width="90%"}

# Common Probability Distributions

## A Bernouli Trial {.smaller}

![](images/week02_74_coin_flip_probability.jpeg){fig-align="center" width="60%"}

## Probability of Independent Events {.smaller}

![](images/week02_75_card_deck_probability_union.jpeg){fig-align="center" width="100%"}

## Bernoulli Distribution {.smaller}

Describes the expected outcome of a single event with probability $p$.

**Example: Flipping a fair coin once**

$$Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p$$

$$Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p = q$$

Probabilities always sum to 1: $p + q = 1$

## Let's Simulate Coin Flips {.smaller}

```{r}
#| label: coin-flip-output
#| echo: false
#| eval: true
#| fig-width: 6
#| fig-height: 4

coin <- c("heads", "tails")
flips <- sample(coin, prob = c(0.5, 0.5), size = 100, replace = TRUE)
barplot(table(flips), col = c("steelblue", "coral"))
```

## Coin Flip Code {.smaller}

```{r}
#| label: coin-flip-code
#| echo: true
#| eval: false

coin <- c("heads", "tails")
flips <- sample(coin, prob = c(0.5, 0.5), size = 100, replace = TRUE)
barplot(table(flips), col = c("steelblue", "coral"))
```

## In-Class Demo: Law of Large Numbers with Coin Flips {.smaller}

Watch how the proportion of heads converges to 0.5 as we flip more coins:

```{r}
#| label: lln-coins-output
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4

set.seed(344)
par(mfrow = c(2, 3))

# Different sample sizes
for(n in c(1, 8, 16, 32, 400, 1000)) {
  flips <- rbinom(n = n, size = 1, prob = 0.5)  # 0 = heads, 1 = tails
  prop_heads <- sum(flips == 0) / length(flips)
  prop_tails <- 1 - prop_heads

  barplot(height = c(prop_heads, prop_tails),
          names.arg = c("heads", "tails"),
          col = c("steelblue", "coral"),
          ylim = c(0, 1),
          main = paste("n =", n, "flips"),
          ylab = "Proportion")
  abline(h = 0.5, lty = 2, col = "gray")
}
```

## LLN Coin Flips Code {.smaller}

```{r}
#| label: lln-coins-code
#| echo: true
#| eval: false

set.seed(344)
par(mfrow = c(2, 3))

# Different sample sizes
for(n in c(1, 8, 16, 32, 400, 1000)) {
  flips <- rbinom(n = n, size = 1, prob = 0.5)  # 0 = heads, 1 = tails
  prop_heads <- sum(flips == 0) / length(flips)
  prop_tails <- 1 - prop_heads

  barplot(height = c(prop_heads, prop_tails),
          names.arg = c("heads", "tails"),
          col = c("steelblue", "coral"),
          ylim = c(0, 1),
          main = paste("n =", n, "flips"),
          ylab = "Proportion")
  abline(h = 0.5, lty = 2, col = "gray")
}
```

::: callout-tip
As sample size increases, observed proportions converge to true probability (0.5).
:::

## Law of Large Numbers in R {.smaller}

```{r}
#| label: fig-lln
#| fig-cap: "Sample means converge to population mean as sample size increases"
#| fig-width: 9
#| fig-height: 4
#| echo: true

# Simulate cumulative means from a Normal(10, 2) population
set.seed(123)
n <- 1000
samples <- rnorm(n, mean = 10, sd = 2)
cumulative_means <- cumsum(samples) / 1:n

plot(1:n, cumulative_means, type = "l", col = "steelblue", lwd = 2,
     xlab = "Sample Size (n)", ylab = "Cumulative Mean",
     main = "Law of Large Numbers: Mean Converges to μ = 10")
abline(h = 10, col = "red", lwd = 2, lty = 2)
legend("topright", c("Cumulative Mean", "True Mean (μ = 10)"),
       col = c("steelblue", "red"), lwd = 2, lty = c(1, 2))
```

## Binomial Distribution {.smaller}

Results from combining several independent Bernoulli events.

$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

Where:

-   $n$ = total number of trials
-   $k$ = number of successes
-   $p$ = probability of success

## Binomial Distribution {.smaller}

![](images/week02_76_bell_shaped_histogram.jpeg){fig-align="center" width="100%"}

## Creating a Binomial Distributions {.smaller}

```{r}
#| label: binom-test-output
#| echo: false
#| eval: true
#| fig-width: 7
#| fig-height: 4

# Plot distribution
plot(0:10, dbinom(x = 0:10, size = 10, prob = 0.5),
     type = "h", lwd = 3, col = "steelblue",
     xlab = "Number of Successes", ylab = "Probability")
```

## Binomial Distribution Code {.smaller}

```{r}
#| label: binom-test-code
#| echo: true
#| eval: false

# Probability of exactly 5 successes in 10 trials
dbinom(x = 5, size = 10, prob = 0.5)

# Plot distribution
plot(0:10, dbinom(x = 0:10, size = 10, prob = 0.5),
     type = "h", lwd = 3, col = "steelblue",
     xlab = "Number of Successes", ylab = "Probability")
```

## Poisson Distribution {.smaller}

For discrete counts (e.g., snails per plot, neuron firings per second).

$$Pr(Y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

::: callout-note
The Poisson is a single-parameter distribution: $\mu = \sigma^2 = \lambda$

Variables with variance \> mean are called "overdispersed" (common in RNA-seq data).
:::

## Poisson Distribution Examples {.smaller}

::::: columns
::: {.column width="50%"}
![Gene length by 500 nt bins](images/week02_77_gene_length_by_500_nt_bins.jpeg){fig-align="center"}
:::

::: {.column width="50%"}
![Increasing λ values](images/week02_78_increasing_λ_values.jpeg){fig-align="center"}
:::
:::::

## Creating Poisson Distributions {.smaller}

```{r}
#| label: pois-test-output
#| echo: false
#| eval: true
#| fig-width: 7
#| fig-height: 4

# Plot distribution
plot(0:15, dpois(x = 0:15, lambda = 3),
     type = "h", lwd = 3, col = "darkgreen",
     xlab = "Count", ylab = "Probability")
```

## Poisson Distribution Code {.smaller}

```{r}
#| label: pois-test-code
#| echo: true
#| eval: false

# Probability of 2 counts given lambda = 1
dpois(x = 2, lambda = 1)

# Plot distribution
plot(0:15, dpois(x = 0:15, lambda = 3),
     type = "h", lwd = 3, col = "darkgreen",
     xlab = "Count", ylab = "Probability")
```

## Geometric Distribution {.smaller}

Probability of observing $k$ trials before the first success:

$$P(X=k)=(1-p)^{k-1}p$$

-   Mean = $\frac{1}{p}$
-   Variance = $\frac{(1-p)}{p^2}$

**Example:** If extinction probability is 0.1 per year, expected time to extinction?

## Negative Binomial Distribution {.smaller}

Probability of the $r^{th}$ success on the $k^{th}$ trial:

$$P(X=k)=\binom{k-1}{r-1}p^{r}(1-p)^{k-r}$$

-   Mean = $\frac{r}{p}$
-   Variance = $\frac{r(1-p)}{p^2}$

# Continuous Probability Distributions {background-color="#2c3e50"}

## Continuous Probability Distributions {.smaller}

$$P(a\leq X \leq b) = \int_{a}^{b} f(x) dx$$

The indefinite integral sums to one:

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

**Expectation:** $$E[X] = \int_{-\infty}^{\infty} xf(x) dx$$

## Uniform Distribution {.smaller}

All outcomes equally probable.

$$E[X] = \frac{(a+b)}{2}$$

```{r}
#| label: unif-dist-output
#| echo: false
#| eval: true
#| fig-width: 6
#| fig-height: 3

x <- seq(0, 10, 0.1)
plot(x, dunif(x, 0, 10), type = "l", lwd = 2, col = "purple",
     ylab = "Density", main = "Uniform Distribution (0, 10)")
```

## Uniform Distribution Code {.smaller}

```{r}
#| label: unif-dist-code
#| echo: true
#| eval: false

x <- seq(0, 10, 0.1)
plot(x, dunif(x, 0, 10), type = "l", lwd = 2, col = "purple",
     ylab = "Density", main = "Uniform Distribution (0, 10)")
```

## Exponential Distribution {.smaller}

$$f(x)=\lambda e^{-\lambda x}$$

-   $E[X] = \frac{1}{\lambda}$
-   $Var(X) = \frac{1}{\lambda^2}$

**Example:** If λ equals the instantaneous death rate, the lifespan follows an exponential distribution.

## Exponential Distribution {.smaller}

```{r}
#| label: exp-dist-output
#| echo: false
#| eval: true
#| fig-width: 7
#| fig-height: 4

x <- seq(0, 50, 0.5)
plot(x, dexp(x, rate = 0.1), type = "l", lwd = 2, col = "red",
     ylab = "Density", main = "Exponential Distribution (λ = 0.1)")
```

## Exponential Distribution Code {.smaller}

```{r}
#| label: exp-dist-code
#| echo: true
#| eval: false

x <- seq(0, 50, 0.5)
plot(x, dexp(x, rate = 0.1), type = "l", lwd = 2, col = "red",
     ylab = "Density", main = "Exponential Distribution (λ = 0.1)")
```

## Gamma Distribution {.smaller}

Waiting time until the $r^{th}$ event at rate $\lambda$:

$$f(x) = \frac{\lambda^r x^{r-1} e^{-\lambda x}}{(r-1)!}$$

-   Mean = $\frac{r}{\lambda}$
-   Variance = $\frac{r}{\lambda^2}$

**Example:** Time until 1000 DNA strands synthesized at rate 1/ms.

# The Normal or Gaussian Distribution

## Normal (Gaussian) Distribution {.smaller}

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \, \mathrm{e}^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

Notation: $v \sim \mathcal{N}(\mu, \sigma^2)$

## Normal Distribution

![](images/week02_79_normal_distribution_68_95_99.jpeg){fig-align="center" width="90%"}

## Normal Distribution {.smaller}

![](images/week02_81_normal_distributions_varying_params.jpeg){fig-align="center" width="50%"}

## Why is the Normal Distribution Special? {.smaller}

![](images/week02_82_living_histogram_height.jpeg){fig-align="center" width="100%"}

## Why Normal is Special in Biology {.smaller}

![](images/week02_83_galton_regression_to_mean.jpeg){fig-align="center" width="70%"}

## Why Normal is Special in Biology {.smaller}

![](images/week02_84_birth_data_visualization_art.jpeg){fig-align="center" width="100%"}

## Z-Scores of Normal distributions {.smaller}

Standardize variables to mean = 0, SD = 1:

$$\huge z_i = \frac{(x_i - \bar{x})}{s}$$

This is the **standard normal distribution**.

# Sampling from distributions in R

## Distribution Functions in R {.smaller}

| Prefix | Function                 | Example              |
|:-------|:-------------------------|:---------------------|
| `d`    | Probability density/mass | `dnorm(0, 0, 1)`     |
| `p`    | Cumulative distribution  | `pnorm(1.96, 0, 1)`  |
| `q`    | Quantile function        | `qnorm(0.975, 0, 1)` |
| `r`    | Random number generator  | `rnorm(100, 0, 1)`   |

Works for: `binom`, `pois`, `exp`, `norm`, `geom`, `nbinom`, `unif`, `gamma`

# Estimates and Confidence Intervals {background-color="#2c3e50"}

## Parameter Estimation {.smaller}

-   Estimation infers population parameters from sample data
-   Sample estimates rarely equal population parameters exactly
-   **Sampling distribution**: all values we might obtain from samples
-   **Standard error**: standard deviation of sampling distribution

::: callout-important
NO ESTIMATE IS USEFUL WITHOUT A STANDARD ERROR!
:::

## Estimation Approaches {.smaller}

| Approach | Description |
|:---|:---|
| **Parametric** | Assumes specific distributions |
| **Resampling** | Bootstrap/randomization for empirical distributions |
| **OLS** | Ordinary Least Squares optimization |
| **Maximum Likelihood** | Model-based estimates with confidence |
| **Bayesian** | Incorporates prior information |

## The Central Limit Theorem {.smaller}

For most data, we can't determine sampling distributions empirically.

The CLT tells us that the sampling distribution of the mean approaches normal as sample size increases, regardless of the underlying distribution.

## Visualizing the Central Limit Theorem {.smaller}

```{r}
#| label: fig-clt-figure
#| fig-cap: "CLT: Sample means become normal even from non-normal populations"
#| fig-width: 10
#| fig-height: 5
#| echo: false
#| eval: true

par(mfrow = c(2, 4))

# Exponential distribution (skewed)
set.seed(42)
exp_pop <- rexp(10000, rate = 1)
hist(exp_pop, main = "Population\n(Exponential)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

# Sample means for different n
for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(exp_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "steelblue", border = "white", xlab = "Mean", breaks = 25)
}

# Uniform distribution
unif_pop <- runif(10000, 0, 1)
hist(unif_pop, main = "Population\n(Uniform)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(unif_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "coral", border = "white", xlab = "Mean", breaks = 25)
}
```

## Visualizing the Central Limit Theorem {.smaller}

```{r}
#| label: fig-clt-code
#| fig-cap: "CLT: Sample means become normal even from non-normal populations"
#| fig-width: 10
#| fig-height: 5
#| echo: true
#| eval: false

par(mfrow = c(2, 4))

# Exponential distribution (skewed)
set.seed(42)
exp_pop <- rexp(10000, rate = 1)
hist(exp_pop, main = "Population\n(Exponential)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

# Sample means for different n
for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(exp_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "steelblue", border = "white", xlab = "Mean", breaks = 25)
}

# Uniform distribution
unif_pop <- runif(10000, 0, 1)
hist(unif_pop, main = "Population\n(Uniform)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(unif_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "coral", border = "white", xlab = "Mean", breaks = 25)
}
```

## CLT in Quantitative Genetics {.smaller}

**Why are complex traits normally distributed?**

Simulate a trait controlled by 5 genes, each with additive effects:

```{r}
#| label: clt-genetics-output
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4

set.seed(245)
n_individuals <- 100

# Each locus contributes -1, 0, or +1 to phenotype (diploid genotypes)
contributions <- matrix(0, nrow = n_individuals, ncol = 5)
for(locus in 1:5) {
  genotype <- rbinom(n_individuals, size = 2, prob = 0.5)  # 0, 1, or 2 copies
  contributions[, locus] <- ifelse(genotype == 2, 1,
                                    ifelse(genotype == 1, 0, -1))
}

# Sum across loci to get phenotype
phenotype <- rowSums(contributions)

par(mfrow = c(1, 2))
hist(phenotype, col = "steelblue", border = "white",
     main = "5-Locus Trait Distribution",
     xlab = "Phenotype Value", breaks = seq(-6, 6, 1))

# Add environmental noise to make continuous
phenotype_continuous <- phenotype + rnorm(n_individuals, 0, 0.5)
hist(phenotype_continuous, col = "coral", border = "white",
     main = "With Environmental Variation",
     xlab = "Phenotype Value", breaks = 20)
```

## CLT Quantitative Genetics Code {.smaller}

```{r}
#| label: clt-genetics-code
#| echo: true
#| eval: false

set.seed(245)
n_individuals <- 100

# Each locus contributes -1, 0, or +1 to phenotype (diploid genotypes)
contributions <- matrix(0, nrow = n_individuals, ncol = 5)
for(locus in 1:5) {
  genotype <- rbinom(n_individuals, size = 2, prob = 0.5)  # 0, 1, or 2 copies
  contributions[, locus] <- ifelse(genotype == 2, 1,
                                    ifelse(genotype == 1, 0, -1))
}

# Sum across loci to get phenotype
phenotype <- rowSums(contributions)

par(mfrow = c(1, 2))
hist(phenotype, col = "steelblue", border = "white",
     main = "5-Locus Trait Distribution",
     xlab = "Phenotype Value", breaks = seq(-6, 6, 1))

# Add environmental noise to make continuous
phenotype_continuous <- phenotype + rnorm(n_individuals, 0, 0.5)
hist(phenotype_continuous, col = "coral", border = "white",
     main = "With Environmental Variation",
     xlab = "Phenotype Value", breaks = 20)
```

::: callout-note
This is why quantitative traits (height, weight, blood pressure) are approximately normal!
:::

# Sampling variation of a parameter estimate

## Sampling Variation of a Parameter {.smaller}

![](images/week02_88_population_vs_sampling_distribution.jpeg){fig-align="center" width="100%"}

## 

![](images/week02_91_sampling_distribution_of_means.jpeg){fig-align="center" width="100%"}

## Estimation and Confidence Intervals {.smaller}

![](images/week02_90_population_vs_single_sample.jpeg){fig-align="center" width="90%"}

## 

![](images/week02_92_standard_error_confidence_interval.jpeg){fig-align="center" width="100%"}

## Standard Error of the Mean {.smaller}

$$\huge \sigma_{\bar{x}} \approx s_{\bar{x}} = \frac{s}{\sqrt{n}}$$

::: callout-note
-   SEM is NOT the standard deviation of the original distribution
-   SEM decreases as sample size increases
:::

## Calculating SEM {.smaller}

```{r}
#| label: sem-calc
#| echo: true

set.seed(32)
true_pop <- rnorm(n = 1000, mean = 2, sd = 5)

# Small sample
samps_5 <- replicate(n = 50, sample(true_pop, size = 5))
sem_5 <- sd(apply(samps_5, 2, mean)) / sqrt(50)

# Larger sample
samps_50 <- replicate(n = 50, sample(true_pop, size = 50))
sem_50 <- sd(apply(samps_50, 2, mean)) / sqrt(50)

cat("SEM (n=5):", round(sem_5, 4), "\nSEM (n=50):", round(sem_50, 4))
```

## Confidence Intervals {.smaller}

A confidence interval is a range of values about a parameter estimate such that we are X% certain the true population parameter lies within.

```{r}
#| label: conf-int
#| echo: true

# 95% CI using t.test
sample_data <- rnorm(30, mean = 10, sd = 2)
t.test(sample_data)$conf.int
```

## Visualizing What "95% Confidence" Means {.smaller}

```{r}
#| label: fig-ci-demo-code
#| fig-cap: "95% of CIs from repeated sampling contain the true mean (μ = 10)"
#| fig-width: 9
#| fig-height: 5
#| echo: false
#| eval: true

true_mean <- 10
n_samples <- 50

# Take 50 samples and calculate CIs
results <- t(replicate(n_samples, {
  samp <- rnorm(20, mean = true_mean, sd = 2)
  ci <- t.test(samp)$conf.int
  c(mean = mean(samp), lower = ci[1], upper = ci[2])
}))

# Plot CIs - color red if they miss the true mean
contains_true <- results[, "lower"] <= true_mean & results[, "upper"] >= true_mean
colors <- ifelse(contains_true, "steelblue", "red")

plot(NULL, xlim = c(8, 12), ylim = c(0, n_samples + 1),
     xlab = "Value", ylab = "Sample Number", main = "50 Confidence Intervals")
abline(v = true_mean, col = "darkgreen", lwd = 2)
for(i in 1:n_samples) {
  segments(results[i, "lower"], i, results[i, "upper"], i, col = colors[i], lwd = 2)
  points(results[i, "mean"], i, pch = 19, cex = 0.6, col = colors[i])
}
legend("topright", c(paste(sum(contains_true), "contain μ"),
       paste(sum(!contains_true), "miss μ")), col = c("steelblue", "red"), lwd = 2)
```

## Visualizing What "95% Confidence" Means {.smaller}

```{r}
#| label: fig-ci-demo-figure
#| fig-cap: "95% of CIs from repeated sampling contain the true mean (μ = 10)"
#| fig-width: 9
#| fig-height: 5
#| echo: true
#| eval: false

true_mean <- 10
n_samples <- 50

# Take 50 samples and calculate CIs
results <- t(replicate(n_samples, {
  samp <- rnorm(20, mean = true_mean, sd = 2)
  ci <- t.test(samp)$conf.int
  c(mean = mean(samp), lower = ci[1], upper = ci[2])
}))

# Plot CIs - color red if they miss the true mean
contains_true <- results[, "lower"] <= true_mean & results[, "upper"] >= true_mean
colors <- ifelse(contains_true, "steelblue", "red")

plot(NULL, xlim = c(8, 12), ylim = c(0, n_samples + 1),
     xlab = "Value", ylab = "Sample Number", main = "50 Confidence Intervals")
abline(v = true_mean, col = "darkgreen", lwd = 2)
for(i in 1:n_samples) {
  segments(results[i, "lower"], i, results[i, "upper"], i, col = colors[i], lwd = 2)
  points(results[i, "mean"], i, pch = 19, cex = 0.6, col = colors[i])
}
legend("topright", c(paste(sum(contains_true), "contain μ"),
       paste(sum(!contains_true), "miss μ")), col = c("steelblue", "red"), lwd = 2)
```

## Coefficient of Variation {.smaller}

To compare standard deviations across populations with different means:

$$CV = \frac{s}{\bar{x}} \times 100\%$$
