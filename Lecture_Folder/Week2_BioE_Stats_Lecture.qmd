---
title: "Statistics for Bioengineering 2026 - Week 2"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2026 - Knight Campus 
    transition: fade
    transition-speed: slow
    theme: default
    slide-number: true
    code-fold: false
    code-overflow: wrap
    highlight-style: github
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(knitr)
library(readxl)
library(MASS)
library(pwr)
library(boot)
theme_set(theme_minimal())
```

# Markdown and Reproducible Research {background-color="#2c3e50"}

## What is Markdown? {.smaller}

-   Lightweight markup language for formatting plaintext documents
-   Add basic syntax to make elements look different when rendered
-   Renders to multiple output formats: PDF, HTML, presentations, websites, books

## Formatting Text

```{r}
#| eval: false
#| echo: true

*Italic* or _Italic_
**Bold** or __Bold__
```

Results: *Italic*, **Bold**

## Block Quotes {.smaller}

```{r}
#| eval: false
#| echo: true

> "You know the greatest danger facing us is ourselves, 
> an irrational fear of the unknown."
>
> --- Captain James T. Kirk
```

> "You know the greatest danger facing us is ourselves, an irrational fear of the unknown."
>
> --- Captain James T. Kirk

## What is Quarto? {.smaller}

-   Open-source scientific and technical publishing system
-   Combines code + text + outputs in a single document
-   Successor to R Markdown with expanded capabilities
-   Supports R, Python, Julia, and more

::: callout-tip
## Key Benefits

Reproducibility, documentation, multiple output formats
:::

## What is LaTeX? {.smaller}

-   Pronounced "Lah-tech" or "Lay-tech"
-   Document preparation system for high-quality typesetting
-   Allows precise mathematical statements
-   Can be included directly in Markdown documents

::: callout-tip
More info: <https://www.latex-project.org>
:::

## LaTeX Operators and Symbols {.smaller}

**Powers, roots, vectors:**

`a^x, \sqrt[n]{x}, \vec{\jmath}, \tilde{\imath}`

$$ \large a^x, \sqrt[n]{x}, \vec{\jmath}, \tilde{\imath}$$

**Greek letters:**

`\alpha, \beta, \gamma, \mu, \sigma, \lambda` $$ \large \alpha, \beta, \gamma, \mu, \sigma, \lambda$$

**Relational operators:** `\approx, \neq, \nsim, \leq, \geq` $$ \large\approx, \neq, \nsim, \leq, \geq $$

## Key Equations in Statistics {.smaller}

**Binomial:** `f(k) = {n \choose k} p\^{k} (1-p)\^{n-k}` $$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

**Poisson:** `Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}` $$\large Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$

**Normal:** `f(x) = \frac{1}{\sqrt{2\pi\sigma}} , \mathrm{e}\^{-\frac{(x - \mu)^2}{2\sigma^2}}` $$f(x) = \frac{1}{\sqrt{2\pi\sigma}} \, \mathrm{e}^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

## Inline vs. Display Equations

**Inline:** `$y=\frac{1}{2}$` renders as $y=\frac{1}{2}$ within text.

**Display:** `$$y=\frac{1}{2}$$` renders on its own line:

$$y=\frac{1}{2}$$

## Code Chunk Options {.smaller}

| Option              | Description           |
|:--------------------|:----------------------|
| `#| echo: true`     | Show code in output   |
| `#| eval: false`    | Don't run the code    |
| `#| output: false`  | Hide results          |
| `#| warning: false` | Hide warnings         |
| `#| fig-cap: "..."` | Figure caption        |
| `#| fig-width: 8`   | Figure width (inches) |

------------------------------------------------------------------------

# Week 2: EDA, Probability, and Experimental Design {background-color="#2c3e50"}

## Week 2 Topics

::: incremental
-   Exploratory data analysis
-   Data visualization
-   Parameters & statistics
-   Probability distributions
-   Estimates & confidence intervals
-   Clinical trials & experimental design
:::

::: callout-note
**Readings:** Chapters 9-15
:::

# Exploratory Data Analysis {background-color="#2c3e50"}

## What is EDA? {.smaller}

-   First step in any data analysis
-   Understand structure and patterns in your data
-   Identify outliers, missing values, errors
-   Generate hypotheses for formal testing

::: callout-tip
## Always visualize your data before running statistical tests!
:::

# Data Wrangling with Tidyverse {background-color="#2c3e50"}

## Tidyverse Family of Packages {.smaller}

![](images/week_3.011.jpeg){fig-align="center" width="90%"}

```{r}
#| label: load-tidyverse
#| echo: true
#| output: false

library(tidyverse)
```

## Key dplyr Verbs {.smaller}

| Verb          | Purpose                | Example                         |
|:--------------|:-----------------------|:--------------------------------|
| `filter()`    | Subset rows by values  | `filter(df, x > 5)`             |
| `select()`    | Subset columns by name | `select(df, col1, col2)`        |
| `arrange()`   | Reorder rows           | `arrange(df, x)`                |
| `mutate()`    | Create new columns     | `mutate(df, z = x + y)`         |
| `summarise()` | Collapse to summary    | `summarise(df, mean = mean(x))` |

## Filter, Arrange, and Select {.smaller}

```{r}
#| eval: false
#| echo: true

# Filter rows
filter(flights, month == 11 | month == 12)

# Arrange rows
arrange(flights, year, month, day)

# Select columns
select(flights, year, month, day)
```

![](images/week_3.015.jpeg){fig-align="center" width="50%"}

## Conditional Operators {.smaller}

| Operator  | Meaning                 |
|:----------|:------------------------|
| `==`      | Equals exactly          |
| `<`, `<=` | Less than (or equal)    |
| `>`, `>=` | Greater than (or equal) |
| `!=`      | Not equal to            |
| `!`       | NOT operator            |
| `&`       | AND operator            |
| `|`       | OR operator             |
| `%in%`    | Belongs to set          |

## Mutate and Transmute {.smaller}

```{r}
#| eval: false
#| echo: true

# Add new columns based on existing ones
mutate(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)

# Using pipes
flights |>
  mutate(
    gain = arr_delay - dep_delay,
    speed = distance / air_time * 60
  )
```

## Group By and Summarise {.smaller}

```{r}
#| eval: false
#| echo: true

# Group by categorical variable
by_day <- group_by(flights, year, month, day)

# Summarise within groups
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

::: callout-warning
Aggregation functions return NA if any input value is NA. Use `na.rm = TRUE` to remove missing values.
:::

## Other Useful dplyr Functions {.smaller}

| Function     | Purpose                  |
|:-------------|:-------------------------|
| `slice()`    | Subset rows by position  |
| `pull()`     | Extract column as vector |
| `count()`    | Count observations       |
| `distinct()` | Unique observations      |

## R Exercise: Data Wrangling {.smaller}

::: callout-tip
## Exercise

1.  Read in `Week1b_Stickle_RNAseq.tsv` or `knee_injury.csv` dataset
2.  `Select` a subset of categorical variables + quantitative variables
3.  `Mutate` to create square root transformed versions
4.  `Summarise` mean and SD `grouped by` categories (such as sex, population, treatment)
5.  Write results to a `.csv` file
:::

# BREAK {background-color="#e74c3c"}

# Data Visualization with ggplot2 {background-color="#2c3e50"}

## Introduction to ggplot2 {.smaller}

-   Part of the `tidyverse` suite of packages
-   GG stands for "Grammar of Graphics"
-   Start with `ggplot()`, supply dataset and aesthetic mapping with `aes()`
-   Add geometry layers with `geom_*()` functions

::: callout-tip
More info: <https://ggplot2.tidyverse.org/>
:::

## Grammar of Graphics

![](images/images_4a.017.jpeg){fig-align="center" width="90%"}

## Boxplots {.smaller}

![](images/boxplot.jpeg){fig-align="center" width="90%"}

## Boxplots {.smaller}

```{r}
#| label: ggplot-boxplot
#| echo: true
#| fig-width: 10
#| fig-height: 4

ggplot(mpg, aes(manufacturer, hwy, colour = class)) + 
  geom_boxplot() + 
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

## Scatterplots {.smaller}

```{r}
#| label: ggplot-scatter
#| echo: true
#| fig-width: 8
#| fig-height: 4

ggplot(mpg, aes(displ, hwy, color = class)) + 
  geom_point(size = 4, alpha = 0.6)
```

## Common ggplot Geoms {.smaller}

| Geom               | Purpose      | Data Type               |
|:-------------------|:-------------|:------------------------|
| `geom_point()`     | Scatterplots | Two continuous          |
| `geom_line()`      | Line plots   | Continuous over ordered |
| `geom_bar()`       | Bar charts   | Categorical counts      |
| `geom_histogram()` | Histograms   | Single continuous       |
| `geom_boxplot()`   | Boxplots     | Continuous by category  |
| `geom_smooth()`    | Trend lines  | Two continuous          |

## Combining Geoms {.smaller}

```{r}
#| label: combine-geoms
#| echo: true
#| fig-width: 8
#| fig-height: 4

ggplot(data=mpg, aes(x=displ, y=hwy)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Fuel efficiency decreases with engine size",
       caption = "Data from fueleconomy.gov")
```

## Faceting {.smaller}

```{r}
#| label: facet-example
#| echo: true
#| fig-width: 10
#| fig-height: 4

ggplot(data=mpg) +
  geom_point(mapping=aes(x=displ, y=hwy)) +
  facet_wrap(~class, nrow=2)
```

## Three-Dimensional Data {.smaller}

```{r}
#| label: 3d-contour
#| echo: true
#| fig-width: 6
#| fig-height: 5

set.seed(345)
d <- data.frame(a = rnorm(100, 10, 10), b = rnorm(100, 5, 5))
ggplot(d, aes(x=a, y=b)) +
  geom_density2d_filled() +
  theme_minimal()
```

## Choosing the Right Plot

![Flow chart for choosing data visualization types](images/Chart_flow_chart.jpeg){fig-align="center" width="90%"}

## R Exercise: Data Visualization {.smaller}

::: callout-tip
## Exercise

1.  Read in `Week1b_Stickle_RNAseq.tsv` or `knee_injury.csv` dataset
2.  Create `histograms` of data
3.  Create histograms on a `faceted` figure for different levels of a category
4.  Repeat steps 2 and 3 but create `boxplots`
5.  Write plots to a `.pdf` file
:::

# Best Practices in Data Visualization {background-color="#2c3e50"}

## Principles of Effective Display

> "Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space"
>
> --- Edward Tufte

## Principles of Effective Display {.smaller}

-   Show the data
-   Encourage the eye to compare differences
-   Represent magnitudes honestly and accurately
-   Draw graphical elements clearly, minimizing clutter
-   Make displays easy to interpret

## "Above All Else Show the Data" {.smaller}

![Tufte 1983](images/images_4a.010.jpeg){fig-align="center" width="80%"}

## "Maximize the Data to Ink Ratio" {.smaller}

![The Economist 2006](images/images_4a.011.jpeg){fig-align="center" width="80%"}

## Represent Magnitudes Honestly {.smaller}

![Rattenborg et al. 1999 Nature](images/images_4a.012.jpeg){fig-align="center" width="70%"}

## How NOT to Make a Figure {.smaller}

::::: columns
::: {.column width="50%"}
![](images/images_4a.013.jpeg){fig-align="center"}
:::

::: {.column width="50%"}
![](images/images_4a.014.jpeg){fig-align="center"}
:::
:::::

"Graphical excellence begins with telling the truth about the data" – Tufte 1983

# Parameters and Statistics {background-color="#2c3e50"}

## Population vs. Sample {.smaller}

| Concept    | Population           | Sample               |
|:-----------|:---------------------|:---------------------|
| Definition | All individuals      | Subset of population |
| Parameters | μ (mean), σ (SD)     | x̄ (mean), s (SD)     |
| Goal       | What we want to know | What we can measure  |

::: callout-important
We use sample statistics to **estimate** population parameters
:::

## Accuracy vs. Precision {.smaller}

![](images/week_2.024.jpeg){fig-align="center" width="50%"}

-   **Accuracy**: closeness to true value
-   **Precision**: closeness of repeated estimates to each other
-   **Replication** quantifies variation
-   **Randomization** avoids bias

## Stochastic Processes in Statistics {.smaller}

-   We often want to know truths about the world, but can only estimate them
-   Uncertainty in those estimates is a given
-   Statistics is largely about quantifying and managing that uncertainty
-   Random variables are products of stochastic processes
-   Different stochastic processes will generate different probability distributions

## Random Variables and Probability {.smaller}

-   **Probability** is the expression of belief in some future outcome
-   A **random variable** can take on different values with different probabilities
-   The **sample space** is the universe of all possible values
-   Sample space represented by:
    -   **Probability mass distribution** (discrete)
    -   **Probability density function** (continuous)
-   Probabilities of a sample space **always sum to 1.0**

## Two Interpretations of Probability {.smaller}

**Frequency interpretation:**

"Probabilities are mathematically convenient approximations to long run relative frequencies."

**Subjective (Bayesian) interpretation:**

"A probability statement expresses the opinion of some individual regarding how certain an event is to occur."

## Probability Rules {.smaller}

**'And' rule (multiplication):** $$Pr(X \text{ and } Y) = Pr(X) \times Pr(Y)$$

**'Or' rule (addition):** $$Pr(X \text{ or } Y) = Pr(X) + Pr(Y)$$

::: callout-note
The 'and' rule assumes independent events. For non-independent events, we need conditional probabilities.
:::

## Joint and Conditional Probability {.smaller}

**Joint probability (independent events):** $$Pr(X,Y) = Pr(X) \times Pr(Y)$$

**Conditional probability (independent):** $$Pr(Y|X) = Pr(Y) \text{ and } Pr(X|Y) = Pr(X)$$

**Conditional probability (non-independent):** $$Pr(Y|X) \neq Pr(Y) \text{ and } Pr(X|Y) \neq Pr(X)$$

## Likelihood vs. Probability {.smaller}

-   **Probability**: proportion of times an event would occur over many trials
-   **Likelihood**: conditional probability of a parameter value given data

$$L[\text{parameter}|\text{data}] = Pr[\text{data}|\text{parameter}]$$

-   **Maximum likelihood**: highest value of the likelihood function
-   **Bayesian estimate**: uses prior distribution to update posterior distribution

## Moments of Distributions {.smaller}

**1st moment (mean/expectation):** $$E[X] = \sum_{\text{all x}}xP(X=x) = \mu$$

**2nd moment (variance):** $$Var(X) = E[(X-\mu)^2] = \sigma^2$$

**Standard deviation:** $$SD = \sqrt{\sigma^2} = \sigma$$

Higher moments include skewness (3rd) and kurtosis (4th).

# Discrete Probability Distributions {background-color="#2c3e50"}

## Bernoulli Distribution {.smaller}

Describes the expected outcome of a single event with probability $p$.

**Example: Flipping a fair coin once**

$$Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p$$

$$Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p = q$$

Probabilities always sum to 1: $p + q = 1$

## Let's Simulate Coin Flips {.smaller}

```{r}
#| label: coin-flip
#| echo: true
#| fig-width: 6
#| fig-height: 4

coin <- c("heads", "tails")
flips <- sample(coin, prob = c(0.5, 0.5), size = 100, replace = TRUE)
barplot(table(flips), col = c("steelblue", "coral"))
```

## Binomial Distribution {.smaller}

Results from combining several independent Bernoulli events.

$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

Where:

-   $n$ = total number of trials
-   $k$ = number of successes
-   $p$ = probability of success

## Binomial Distribution {.smaller}

![](images/week_2.003.jpeg){fig-align="center" width="100%"}

## Testing Binomial Distributions {.smaller}

```{r}
#| label: binom-test
#| echo: true
#| fig-width: 7
#| fig-height: 4

# Probability of exactly 5 successes in 10 trials
dbinom(x = 5, size = 10, prob = 0.5)

# Plot distribution
plot(0:10, dbinom(x = 0:10, size = 10, prob = 0.5), 
     type = "h", lwd = 3, col = "steelblue",
     xlab = "Number of Successes", ylab = "Probability")
```

## Poisson Distribution {.smaller}

-   For discrete counts (e.g., snails per plot, neuron firings per second).

$$Pr(Y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

-   The Poisson is a single-parameter distribution: $\mu = \sigma^2 = \lambda$

-   Variables with variance \> mean are called "overdispersed" (common in RNA-seq data).

## Poisson Distribution Examples {.smaller}

::::: columns
::: {.column width="50%"}
![Gene length by 500 nt bins](images/week_2.004.jpeg){fig-align="center"}
:::

::: {.column width="50%"}
![Increasing λ values](images/week_2.005.jpeg){fig-align="center"}
:::
:::::

## Testing Poisson Distributions {.smaller}

```{r}
#| label: pois-test
#| echo: true
#| fig-width: 7
#| fig-height: 4

# Probability of 2 counts given lambda = 1
dpois(x = 2, lambda = 1)

# Plot distribution
plot(0:15, dpois(x = 0:15, lambda = 3), 
     type = "h", lwd = 3, col = "darkgreen",
     xlab = "Count", ylab = "Probability")
```

## Geometric Distribution {.smaller}

Probability of observing $k$ trials before the first success:

$$P(X=k)=(1-p)^{k-1}p$$

-   Mean = $\frac{1}{p}$
-   Variance = $\frac{(1-p)}{p^2}$

**Example:** If extinction probability is 0.1 per year, expected time to extinction?

## Negative Binomial Distribution {.smaller}

Probability of the $r^{th}$ success on the $k^{th}$ trial:

$$P(X=k)=\binom{k-1}{r-1}p^{r}(1-p)^{k-r}$$

-   Mean = $\frac{r}{p}$
-   Variance = $\frac{r(1-p)}{p^2}$

# Continuous Probability Distributions {background-color="#2c3e50"}

## Continuous Probability Distributions {.smaller}

$$P(a\leq X \leq b) = \int_{a}^{b} f(x) dx$$

The indefinite integral sums to one:

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

**Expectation:** $$E[X] = \int_{-\infty}^{\infty} xf(x) dx$$

## Uniform Distribution {.smaller}

All outcomes equally probable.

$$E[X] = \frac{(a+b)}{2}$$

```{r}
#| label: unif-dist
#| echo: true
#| fig-width: 6
#| fig-height: 3

x <- seq(0, 10, 0.1)
plot(x, dunif(x, 0, 10), type = "l", lwd = 2, col = "purple",
     ylab = "Density", main = "Uniform Distribution (0, 10)")
```

## Exponential Distribution {.smaller}

$$f(x)=\lambda e^{-\lambda x}$$

-   $E[X] = \frac{1}{\lambda}$
-   $Var(X) = \frac{1}{\lambda^2}$

**Example:** If λ equals the instantaneous death rate, the lifespan follows an exponential distribution.

## Exponential Distribution {.smaller}

```{r}
#| label: exp-dist
#| echo: true
#| fig-width: 7
#| fig-height: 4

x <- seq(0, 50, 0.5)
plot(x, dexp(x, rate = 0.1), type = "l", lwd = 2, col = "red",
     ylab = "Density", main = "Exponential Distribution (λ = 0.1)")
```

## Gamma Distribution {.smaller}

Waiting time until the $r^{th}$ event at rate $\lambda$:

$$f(x) = \frac{\lambda^r x^{r-1} e^{-\lambda x}}{(r-1)!}$$

-   Mean = $\frac{r}{\lambda}$
-   Variance = $\frac{r}{\lambda^2}$

**Example:** Time until 1000 DNA strands synthesized at rate 1/ms.

## Normal (Gaussian) Distribution {.smaller}

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \, \mathrm{e}^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

Notation: $v \sim \mathcal{N}(\mu, \sigma^2)$

## 

![](images/week_2.011.jpeg){fig-align="center" width="90%"}

## Why is the Normal Distribution Special? {.smaller}

![](images/week_2.013.jpeg){fig-align="center" width="100%"}

## Central Limit Theorem Connection {.smaller}

![](images/week_2.015.jpeg){fig-align="center" width="100%"}

## Z-Scores {.smaller}

Standardize variables to mean = 0, SD = 1:

$$\huge z_i = \frac{(x_i - \bar{x})}{s}$$

This is the **standard normal distribution**.

## Distribution Functions in R {.smaller}

| Prefix | Function                 | Example              |
|:-------|:-------------------------|:---------------------|
| `d`    | Probability density/mass | `dnorm(0, 0, 1)`     |
| `p`    | Cumulative distribution  | `pnorm(1.96, 0, 1)`  |
| `q`    | Quantile function        | `qnorm(0.975, 0, 1)` |
| `r`    | Random number generator  | `rnorm(100, 0, 1)`   |

Works for: `binom`, `pois`, `exp`, `norm`, `geom`, `nbinom`, `unif`, `gamma`

# Estimates and Confidence Intervals {background-color="#2c3e50"}

## Parameter Estimation {.smaller}

-   Estimation infers population parameters from sample data
-   Sample estimates rarely equal population parameters exactly
-   **Sampling distribution**: all values we might obtain from samples
-   **Standard error**: standard deviation of sampling distribution

::: callout-important
NO ESTIMATE IS USEFUL WITHOUT A STANDARD ERROR!
:::

## Estimation Approaches {.smaller}

| Approach | Description |
|:---|:---|
| **Parametric** | Assumes specific distributions |
| **Resampling** | Bootstrap/randomization for empirical distributions |
| **OLS** | Ordinary Least Squares optimization |
| **Maximum Likelihood** | Model-based estimates with confidence |
| **Bayesian** | Incorporates prior information |

## The Central Limit Theorem {.smaller}

For most data, we can't determine sampling distributions empirically.

The CLT tells us that the sampling distribution of the mean approaches normal as sample size increases, regardless of the underlying distribution.


## The Central Limit Theorem {.smaller}

For most data, we can't determine sampling distributions empirically.

The CLT tells us that the sampling distribution of the mean approaches normal as sample size increases, regardless of the underlying distribution.

## Visualizing the Central Limit Theorem {.smaller}

```{r}
#| label: fig-clt-figure
#| fig-cap: "CLT: Sample means become normal even from non-normal populations"
#| fig-width: 10
#| fig-height: 5
#| echo: false

par(mfrow = c(2, 4))

# Exponential distribution (skewed)
set.seed(42)
exp_pop <- rexp(10000, rate = 1)
hist(exp_pop, main = "Population\n(Exponential)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

# Sample means for different n
for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(exp_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "steelblue", border = "white", xlab = "Mean", breaks = 25)
}

# Uniform distribution
unif_pop <- runif(10000, 0, 1)
hist(unif_pop, main = "Population\n(Uniform)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(unif_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "coral", border = "white", xlab = "Mean", breaks = 25)
}
```

## Visualizing the Central Limit Theorem {.smaller}

```{r}
#| label: fig-clt-code
#| fig-cap: "CLT: Sample means become normal even from non-normal populations"
#| fig-width: 10
#| fig-height: 5
#| echo: true
#| eval: false

par(mfrow = c(2, 4))

# Exponential distribution (skewed)
set.seed(42)
exp_pop <- rexp(10000, rate = 1)
hist(exp_pop, main = "Population\n(Exponential)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

# Sample means for different n
for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(exp_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "steelblue", border = "white", xlab = "Mean", breaks = 25)
}

# Uniform distribution
unif_pop <- runif(10000, 0, 1)
hist(unif_pop, main = "Population\n(Uniform)", col = "lightgray",
     border = "white", xlab = "Value", breaks = 30)

for(n in c(5, 30, 100)) {
  means <- replicate(1000, mean(sample(unif_pop, n, replace = TRUE)))
  hist(means, main = paste("Sample Means\n(n =", n, ")"),
       col = "coral", border = "white", xlab = "Mean", breaks = 25)
}
```

## In-Class Demo: CLT in Quantitative Genetics {.smaller}

**Why are complex traits normally distributed?**

Simulate a trait controlled by 5 genes, each with additive effects:

```{r}
#| fig-width: 10
#| fig-height: 4
#| eval: false
#| echo: true

set.seed(245)
n_individuals <- 100

# Each locus contributes -1, 0, or +1 to phenotype (diploid genotypes)
contributions <- matrix(0, nrow = n_individuals, ncol = 5)
for(locus in 1:5) {
  genotype <- rbinom(n_individuals, size = 2, prob = 0.5)  # 0, 1, or 2 copies
  contributions[, locus] <- ifelse(genotype == 2, 1,
                                    ifelse(genotype == 1, 0, -1))
}

# Sum across loci to get phenotype
phenotype <- rowSums(contributions)

par(mfrow = c(1, 2))
hist(phenotype, col = "steelblue", border = "white",
     main = "5-Locus Trait Distribution",
     xlab = "Phenotype Value", breaks = seq(-6, 6, 1))

# Add environmental noise to make continuous
phenotype_continuous <- phenotype + rnorm(n_individuals, 0, 0.5)
hist(phenotype_continuous, col = "coral", border = "white",
     main = "With Environmental Variation",
     xlab = "Phenotype Value", breaks = 20)
```


## In-Class Demo: CLT in Quantitative Genetics {.smaller}

```{r}
#| fig-width: 10
#| fig-height: 4
#| eval: true
#| echo: false

set.seed(245)
n_individuals <- 100

# Each locus contributes -1, 0, or +1 to phenotype (diploid genotypes)
contributions <- matrix(0, nrow = n_individuals, ncol = 5)
for(locus in 1:5) {
  genotype <- rbinom(n_individuals, size = 2, prob = 0.5)  # 0, 1, or 2 copies
  contributions[, locus] <- ifelse(genotype == 2, 1,
                                    ifelse(genotype == 1, 0, -1))
}

# Sum across loci to get phenotype
phenotype <- rowSums(contributions)

par(mfrow = c(1, 2))
hist(phenotype, col = "steelblue", border = "white",
     main = "5-Locus Trait Distribution",
     xlab = "Phenotype Value", breaks = seq(-6, 6, 1))

# Add environmental noise to make continuous
phenotype_continuous <- phenotype + rnorm(n_individuals, 0, 0.5)
hist(phenotype_continuous, col = "coral", border = "white",
     main = "With Environmental Variation",
     xlab = "Phenotype Value", breaks = 20)
```

::: callout-note
This is why quantitative traits (height, weight, blood pressure) are approximately normal!
:::

## Sampling Variation of a Parameter {.smaller}

![](images/week02_88_slide025.jpeg){fig-align="center" width="100%"}

## Normal Sampling Distribution {.smaller}

![](images/week02_89_week7_normalsampling.jpeg){fig-align="center" width="90%"}

## Estimation and Confidence Intervals {.smaller}

![](images/week02_90_slide026.jpeg){fig-align="center" width="90%"}

## 

![](images/week02_91_slide027.jpeg){fig-align="center" width="100%"}

## 

![](images/week02_92_slide028.jpeg){fig-align="center" width="100%"}


## Standard Error of the Mean {.smaller}

$$\huge \sigma_{\bar{x}} \approx s_{\bar{x}} = \frac{s}{\sqrt{n}}$$

::: callout-note
-   SEM is NOT the standard deviation of the original distribution
-   SEM decreases as sample size increases
:::

## Calculating SEM {.smaller}

```{r}
#| label: sem-calc
#| echo: true

set.seed(32)
true_pop <- rnorm(n = 1000, mean = 2, sd = 5)

# Small sample
samps_5 <- replicate(n = 50, sample(true_pop, size = 5))
sem_5 <- sd(apply(samps_5, 2, mean)) / sqrt(50)

# Larger sample
samps_50 <- replicate(n = 50, sample(true_pop, size = 50))
sem_50 <- sd(apply(samps_50, 2, mean)) / sqrt(50)

cat("SEM (n=5):", round(sem_5, 4), "\nSEM (n=50):", round(sem_50, 4))
```

## Confidence Intervals {.smaller}

A confidence interval is a range of values about a parameter estimate such that we are X% certain the true population parameter lies within.

```{r}
#| label: conf-int
#| echo: true

# 95% CI using t.test
sample_data <- rnorm(30, mean = 10, sd = 2)
t.test(sample_data)$conf.int
```


## Visualizing What "95% Confidence" Means {.smaller}

```{r}
#| label: fig-ci-demo-output
#| fig-cap: "95% of CIs from repeated sampling contain the true mean (μ = 10)"
#| fig-width: 9
#| fig-height: 5
#| echo: false
#| eval: true

set.seed(42)
true_mean <- 10
n_samples <- 50

# Take 50 samples and calculate CIs
results <- t(replicate(n_samples, {
  samp <- rnorm(20, mean = true_mean, sd = 2)
  ci <- t.test(samp)$conf.int
  c(mean = mean(samp), lower = ci[1], upper = ci[2])
}))

# Plot CIs - color red if they miss the true mean
contains_true <- results[, "lower"] <= true_mean & results[, "upper"] >= true_mean
colors <- ifelse(contains_true, "steelblue", "red")

plot(NULL, xlim = c(8, 12), ylim = c(0, n_samples + 1),
     xlab = "Value", ylab = "Sample Number", main = "50 Confidence Intervals")
abline(v = true_mean, col = "darkgreen", lwd = 2)
for(i in 1:n_samples) {
  segments(results[i, "lower"], i, results[i, "upper"], i, col = colors[i], lwd = 2)
  points(results[i, "mean"], i, pch = 19, cex = 0.6, col = colors[i])
}
legend("topright", c(paste(sum(contains_true), "contain μ"),
       paste(sum(!contains_true), "miss μ")), col = c("steelblue", "red"), lwd = 2)
```

## Visualizing What "95% Confidence" Means {.smaller}

```{r}
#| label: fig-ci-demo-code
#| fig-cap: "95% of CIs from repeated sampling contain the true mean (μ = 10)"
#| fig-width: 9
#| fig-height: 5
#| echo: true
#| eval: false

set.seed(42)
true_mean <- 10
n_samples <- 50

# Take 50 samples and calculate CIs
results <- t(replicate(n_samples, {
  samp <- rnorm(20, mean = true_mean, sd = 2)
  ci <- t.test(samp)$conf.int
  c(mean = mean(samp), lower = ci[1], upper = ci[2])
}))

# Plot CIs - color red if they miss the true mean
contains_true <- results[, "lower"] <= true_mean & results[, "upper"] >= true_mean
colors <- ifelse(contains_true, "steelblue", "red")

plot(NULL, xlim = c(8, 12), ylim = c(0, n_samples + 1),
     xlab = "Value", ylab = "Sample Number", main = "50 Confidence Intervals")
abline(v = true_mean, col = "darkgreen", lwd = 2)
for(i in 1:n_samples) {
  segments(results[i, "lower"], i, results[i, "upper"], i, col = colors[i], lwd = 2)
  points(results[i, "mean"], i, pch = 19, cex = 0.6, col = colors[i])
}
legend("topright", c(paste(sum(contains_true), "contain μ"),
       paste(sum(!contains_true), "miss μ")), col = c("steelblue", "red"), lwd = 2)
```



# BREAK {background-color="#e74c3c"}

# Experimental Design Principles {background-color="#2c3e50"}

## What is an Experimental Study? {.smaller}

-   In an **experimental study** the researcher assigns treatments to units
-   In an **observational study** nature does the assigning
-   The crucial advantage of experiments: **random assignment of treatments**
-   Randomization **minimizes the influence of confounding variables**
-   Allows us to infer **cause and effect**

## Clinical Trials {.smaller}

-   Gold standard of experimental designs
-   Two or more treatments assigned to human subjects
-   Design refined because cost of mistakes is high

**Key components:**

-   Simultaneous control group
-   Randomization
-   Blinding
-   Replication
-   Balance
-   Blocking

## Clinical Trial Example {.smaller}

![](images/images_6a.005.jpeg){fig-align="center" width="90%"}

## Simultaneous Control Group {.smaller}

-   Placebo or currently accepted treatment
-   Control subjects should be perturbed similarly to treated subjects
-   "Sham operation" example in surgical studies

## Randomization {.smaller}

-   Breaks association between confounding variables and treatment
-   Ensures variation from confounding variables is similar across groups

**Types:**

-   Completely randomized design
-   Randomized block design
-   Matched pair design

## Random Sampling Approaches {.smaller}

-   **Simple random sample** - every sample has equal probability
-   **Stratified sample** - divided into groups, then random sample from each
-   **Cluster sample** - random sample of naturally occurring groups
-   **Multistage sampling** - combines the above approaches
-   **Systematic sample** - predetermined pattern (e.g., every 20th person)

## Blinding {.smaller}

-   **Single-blind:** Subjects unaware of treatment
-   **Double-blind:** Both subjects and researchers unaware

::: callout-important
Studies without double-blinding exaggerate treatment effects by 16% on average (Jüni et al. 2001)
:::

## Replication and Balance {.smaller}

**Replication:**

-   Assignment of each treatment to multiple independent units
-   Larger samples = smaller standard errors, more power

**Balance:**

-   Equal sample sizes across treatments
-   Minimizes standard error

![](images/images_6a.007.jpeg){fig-align="center" width="80%"}

## Pseudoreplication {.smaller}

![](images/images_6a.006.jpeg){fig-align="center" width="90%"}

## Blocking {.smaller}

-   Grouping of experimental units with similar properties
-   Treatments randomly assigned within blocks
-   Reduces variation from differences between blocks

![](images/images_6a.008.jpeg){fig-align="center" width="80%"}

## Paired Designs {.smaller}

![](images/images_6a.009.jpeg){fig-align="center" width="90%"}
